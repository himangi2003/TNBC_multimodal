{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad0c0b2-9983-4acb-99e7-2ea1d2bc8678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|2025-07-18|21:36:45.689| [WARNING] /cluster/home/srivash/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from PIL import Image\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tiatoolbox.tools.patchextraction import SlidingWindowPatchExtractor\n",
    "from tiatoolbox.wsicore.wsireader import WSIReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53281d67-7b0b-48ab-a21a-6fcbe1c8af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e5eefa-3f3d-43a4-88d9-a77e7ed6f78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59b6898ca3e4b42a8a86a8b0a0faeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d0eabb-324f-4e24-948d-ceb2836cb302",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# need to specify MLP layer and activation function for proper init\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf-hub:paige-ai/Virchow2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSwiGLUPacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSiLU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/timm/models/_factory.py:117\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    125\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/timm/models/vision_transformer.py:2309\u001b[0m, in \u001b[0;36mvit_huge_patch14_224\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   2306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).\u001b[39;00m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2308\u001b[0m model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1280\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m-> 2309\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_create_vision_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_huge_patch14_224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/timm/models/vision_transformer.py:2132\u001b[0m, in \u001b[0;36m_create_vision_transformer\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiglip\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m variant \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_pool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   2130\u001b[0m     strict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_model_with_cfg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mVisionTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgetter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/timm/models/_builder.py:427\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m num_classes_pretrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[0;32m--> 427\u001b[0m     \u001b[43mload_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43min_chans\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_strict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Wrap the model in a feature extraction module if enabled\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features:\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/timm/models/_builder.py:252\u001b[0m, in \u001b[0;36mload_pretrained\u001b[0;34m(model, pretrained_cfg, num_classes, in_chans, filter_fn, strict)\u001b[0m\n\u001b[1;32m    249\u001b[0m             classifier_bias \u001b[38;5;241m=\u001b[39m state_dict[classifier_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.bias\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    250\u001b[0m             state_dict[classifier_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.bias\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m classifier_bias[label_offset:]\n\u001b[0;32m--> 252\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_result\u001b[38;5;241m.\u001b[39mmissing_keys:\n\u001b[1;32m    254\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing keys (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(load_result\u001b[38;5;241m.\u001b[39mmissing_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) discovered while loading pretrained weights.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m This is expected if model is being adapted.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/torch/nn/modules/module.py:2564\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2558\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   2559\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2560\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2561\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2562\u001b[0m         )\n\u001b[0;32m-> 2564\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2546\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2547\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2548\u001b[0m             k: v\n\u001b[1;32m   2549\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2550\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2551\u001b[0m         }\n\u001b[0;32m-> 2552\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2546\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2547\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2548\u001b[0m             k: v\n\u001b[1;32m   2549\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2550\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2551\u001b[0m         }\n\u001b[0;32m-> 2552\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "    \u001b[0;31m[... skipping similar frames: Module.load_state_dict.<locals>.load at line 2552 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/torch/nn/modules/module.py:2552\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2546\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2547\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2548\u001b[0m             k: v\n\u001b[1;32m   2549\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   2550\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(child_prefix)\n\u001b[1;32m   2551\u001b[0m         }\n\u001b[0;32m-> 2552\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[1;32m   2555\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/torch/nn/modules/module.py:2535\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[1;32m   2534\u001b[0m     local_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massign_to_params_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m assign\n\u001b[0;32m-> 2535\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/torch/nn/modules/module.py:2441\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2439\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2440\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2441\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2443\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.layers import SwiGLUPacked\n",
    "from PIL import Image\n",
    "\n",
    "# need to specify MLP layer and activation function for proper init\n",
    "model = timm.create_model(\"hf-hub:paige-ai/Virchow2\", pretrained=True, mlp_layer=SwiGLUPacked, act_layer=torch.nn.SiLU)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18868bea-ead6-4015-9a93-3645fee5e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_img_dir = '/lab/deasylab3/Himangi/tnbc_data/roi_imagesroi_TCGA-A1-A0SP-01Z-00-DX1.20D689C6-EFA5-4694-BE76-24475A89ACC0_3000_24862_9000_30896.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd535a1c-4f17-43cc-a7b5-2955018c5ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tiatoolbox.wsicore.wsireader.VirtualWSIReader object at 0x1478fe48df90>\n"
     ]
    }
   ],
   "source": [
    "reader = WSIReader.open(roi_img_dir)\n",
    "print(reader)  # noqa: T201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a67ea7-5ffe-4c88-b50d-ac4ba1a3788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|2025-07-14|11:17:54.118| [WARNING] Raw data is None.\n",
      "|2025-07-14|11:17:54.119| [WARNING] Unknown scale (no objective_power or mpp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done — CSV should be at: /lab/deasylab3/Himangi/TNBC_multimodal/tiles/patch_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "from tiatoolbox.tools.patchextraction import SlidingWindowPatchExtractor\n",
    "from tiatoolbox.utils.misc import imread, imwrite\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# 1. Load image from path\n",
    "input_img = imread(roi_img_dir)  # roi_img_dir must be a valid image path\n",
    "\n",
    "# 2. Patch and stride settings\n",
    "patch_size = (224, 224)\n",
    "stride = (224, 224)\n",
    "\n",
    "# 3. Image dimensions\n",
    "height, width = input_img.shape[:2]\n",
    "\n",
    "# 4. Create extractor\n",
    "extractor = SlidingWindowPatchExtractor(\n",
    "    input_img=input_img,\n",
    "    patch_size=patch_size,\n",
    "    stride=stride,\n",
    ")\n",
    "\n",
    "# 5. Create output folder and CSV\n",
    "os.makedirs(\"tiles\", exist_ok=True)\n",
    "\n",
    "csv_path = \"tiles/patch_metadata.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Index\", \"X\", \"Y\", \"Filename\"])\n",
    "\n",
    "    i = 0\n",
    "    for y in range(0, height - patch_size[1] + 1, stride[1]):\n",
    "        for x in range(0, width - patch_size[0] + 1, stride[0]):\n",
    "            patch = extractor[i]\n",
    "            i += 1\n",
    "\n",
    "            filename = f\"patch_{i:04d}_x{x}_y{y}.png\"\n",
    "            imwrite(os.path.join(\"tiles\", filename), patch)\n",
    "            writer.writerow([i, x, y, filename])\n",
    "\n",
    "print(\"✅ Done — CSV should be at:\", os.path.abspath(csv_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a73fedf-1e38-45d7-86a2-425ce37c0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear logger to use tiatoolbox.logger\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import joblib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tiatoolbox import logger\n",
    "from tiatoolbox.models.engine.nucleus_instance_segmentor import NucleusInstanceSegmentor\n",
    "from tiatoolbox.utils.misc import download_data, imread\n",
    "\n",
    "# We need this function to visualize the nuclear predictions\n",
    "from tiatoolbox.utils.visualization import (\n",
    "    overlay_prediction_contours,\n",
    ")\n",
    "from tiatoolbox.wsicore.wsireader import WSIReader\n",
    "\n",
    "if logging.getLogger().hasHandlers():\n",
    "    logging.getLogger().handlers.clear()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mpl.rcParams[\"figure.dpi\"] = 300  # for high resolution figure in notebook\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"  # To make sure text is visible in dark mode\n",
    "plt.rcParams.update({\"font.size\": 5})\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b243537c-9049-4807-9f47-edf17a01e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# cell segmentation \n",
    "inst_segmentor = NucleusInstanceSegmentor(\n",
    "    pretrained_model=\"hovernet_fast-pannuke\",\n",
    "    num_loader_workers=2,\n",
    "    num_postproc_workers=2,\n",
    "    batch_size=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "418e9140-e6eb-443f-9676-21f05210eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "png_paths = sorted(glob.glob(os.path.join(\"tiles\", \"*.png\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f449b-d302-487a-8637-9b1d8614ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if os.path.exists(TEST_FOLDER):\n",
    "    shutil.rmtree(TEST_FOLDER)\n",
    "os.makedirs(TEST_FOLDER)\n",
    "\n",
    "tile_output = inst_segmentor.predict(\n",
    "    png_paths,\n",
    "    save_dir=\"sample_tile_results3/\",\n",
    "    mode=\"tile\",\n",
    "    device=\"cuda\",\n",
    "    crash_on_exception=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44d4be5d-9858-4d39-a2ad-d8702a66f231",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`save_dir` already exists! /lab/deasylab3/Himangi/TNBC_multimodal/sample_tile_results3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tile_output \u001b[38;5;241m=\u001b[39m \u001b[43minst_segmentor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpng_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_tile_results3/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrash_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox-gpu/lib/python3.10/site-packages/tiatoolbox/models/engine/semantic_segmentor.py:1359\u001b[0m, in \u001b[0;36mSemanticSegmentor.predict\u001b[0;34m(self, imgs, masks, mode, ioconfig, patch_input_shape, patch_output_shape, stride_shape, resolution, units, save_dir, device, crash_on_exception)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid mode. Use either `tile` or `wsi`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 1359\u001b[0m save_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_save_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m ioconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ioconfig(\n\u001b[1;32m   1362\u001b[0m     ioconfig,\n\u001b[1;32m   1363\u001b[0m     mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     units,\n\u001b[1;32m   1369\u001b[0m )\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;66;03m# use external for testing\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox-gpu/lib/python3.10/site-packages/tiatoolbox/models/engine/semantic_segmentor.py:1071\u001b[0m, in \u001b[0;36mSemanticSegmentor._prepare_save_dir\u001b[0;34m(save_dir)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_dir\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m   1070\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`save_dir` already exists! \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1071\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1072\u001b[0m save_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1073\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cache\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: `save_dir` already exists! /lab/deasylab3/Himangi/TNBC_multimodal/sample_tile_results3"
     ]
    }
   ],
   "source": [
    "tile_output = inst_segmentor.predict(\n",
    "    png_paths,\n",
    "    save_dir=\"sample_tile_results3/\",\n",
    "    mode=\"tile\",\n",
    "    device=\"cuda\",\n",
    "    crash_on_exception=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34a9e9-da0e-46df-bf95-3b0e6aad4969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b21c1cf-6396-4b48-8651-0a4da61a976a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"['tiles/patch_0551_x896_y4704.png', '/lab/deasylab3/Himangi/TNBC_multimodal/sample_tile_results3/550'].dat\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tile_preds \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtile_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m550\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.dat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tiatoolbox-gpu/lib/python3.10/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"['tiles/patch_0551_x896_y4704.png', '/lab/deasylab3/Himangi/TNBC_multimodal/sample_tile_results3/550'].dat\""
     ]
    }
   ],
   "source": [
    "tile_preds = joblib.load(f\"{tile_output[550]}.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6fdee19-6509-4c03-87fa-ec1d954b530d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Number of detected nuclei: 32\n"
     ]
    }
   ],
   "source": [
    "tile_preds = joblib.load(f\"{tile_output[555][1]}.dat\")\n",
    "logger.info(f\"Number of detected nuclei: {len(tile_preds)}\")\n",
    "\n",
    "# Extracting the nucleus IDs and select the first one\n",
    "nuc_id_list = list(tile_preds.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eb4f92df-27ca-4f18-8e50-509108ad15b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 676 .dat files (excluding those starting with 'file'):\n",
      "['0.dat', '1.dat', '2.dat', '3.dat', '4.dat', '5.dat', '6.dat', '7.dat', '8.dat', '9.dat', '10.dat', '11.dat', '12.dat', '13.dat', '14.dat', '15.dat', '16.dat', '17.dat', '18.dat', '19.dat', '20.dat', '21.dat', '22.dat', '23.dat', '24.dat', '25.dat', '26.dat', '27.dat', '28.dat', '29.dat', '30.dat', '31.dat', '32.dat', '33.dat', '34.dat', '35.dat', '36.dat', '37.dat', '38.dat', '39.dat', '40.dat', '41.dat', '42.dat', '43.dat', '44.dat', '45.dat', '46.dat', '47.dat', '48.dat', '49.dat', '50.dat', '51.dat', '52.dat', '53.dat', '54.dat', '55.dat', '56.dat', '57.dat', '58.dat', '59.dat', '60.dat', '61.dat', '62.dat', '63.dat', '64.dat', '65.dat', '66.dat', '67.dat', '68.dat', '69.dat', '70.dat', '71.dat', '72.dat', '73.dat', '74.dat', '75.dat', '76.dat', '77.dat', '78.dat', '79.dat', '80.dat', '81.dat', '82.dat', '83.dat', '84.dat', '85.dat', '86.dat', '87.dat', '88.dat', '89.dat', '90.dat', '91.dat', '92.dat', '93.dat', '94.dat', '95.dat', '96.dat', '97.dat', '98.dat', '99.dat', '100.dat', '101.dat', '102.dat', '103.dat', '104.dat', '105.dat', '106.dat', '107.dat', '108.dat', '109.dat', '110.dat', '111.dat', '112.dat', '113.dat', '114.dat', '115.dat', '116.dat', '117.dat', '118.dat', '119.dat', '120.dat', '121.dat', '122.dat', '123.dat', '124.dat', '125.dat', '126.dat', '127.dat', '128.dat', '129.dat', '130.dat', '131.dat', '132.dat', '133.dat', '134.dat', '135.dat', '136.dat', '137.dat', '138.dat', '139.dat', '140.dat', '141.dat', '142.dat', '143.dat', '144.dat', '145.dat', '146.dat', '147.dat', '148.dat', '149.dat', '150.dat', '151.dat', '152.dat', '153.dat', '154.dat', '155.dat', '156.dat', '157.dat', '158.dat', '159.dat', '160.dat', '161.dat', '162.dat', '163.dat', '164.dat', '165.dat', '166.dat', '167.dat', '168.dat', '169.dat', '170.dat', '171.dat', '172.dat', '173.dat', '174.dat', '175.dat', '176.dat', '177.dat', '178.dat', '179.dat', '180.dat', '181.dat', '182.dat', '183.dat', '184.dat', '185.dat', '186.dat', '187.dat', '188.dat', '189.dat', '190.dat', '191.dat', '192.dat', '193.dat', '194.dat', '195.dat', '196.dat', '197.dat', '198.dat', '199.dat', '200.dat', '201.dat', '202.dat', '203.dat', '204.dat', '205.dat', '206.dat', '207.dat', '208.dat', '209.dat', '210.dat', '211.dat', '212.dat', '213.dat', '214.dat', '215.dat', '216.dat', '217.dat', '218.dat', '219.dat', '220.dat', '221.dat', '222.dat', '223.dat', '224.dat', '225.dat', '226.dat', '227.dat', '228.dat', '229.dat', '230.dat', '231.dat', '232.dat', '233.dat', '234.dat', '235.dat', '236.dat', '237.dat', '238.dat', '239.dat', '240.dat', '241.dat', '242.dat', '243.dat', '244.dat', '245.dat', '246.dat', '247.dat', '248.dat', '249.dat', '250.dat', '251.dat', '252.dat', '253.dat', '254.dat', '255.dat', '256.dat', '257.dat', '258.dat', '259.dat', '260.dat', '261.dat', '262.dat', '263.dat', '264.dat', '265.dat', '266.dat', '267.dat', '268.dat', '269.dat', '270.dat', '271.dat', '272.dat', '273.dat', '274.dat', '275.dat', '276.dat', '277.dat', '278.dat', '279.dat', '280.dat', '281.dat', '282.dat', '283.dat', '284.dat', '285.dat', '286.dat', '287.dat', '288.dat', '289.dat', '290.dat', '291.dat', '292.dat', '293.dat', '294.dat', '295.dat', '296.dat', '297.dat', '298.dat', '299.dat', '300.dat', '301.dat', '302.dat', '303.dat', '304.dat', '305.dat', '306.dat', '307.dat', '308.dat', '309.dat', '310.dat', '311.dat', '312.dat', '313.dat', '314.dat', '315.dat', '316.dat', '317.dat', '318.dat', '319.dat', '320.dat', '321.dat', '322.dat', '323.dat', '324.dat', '325.dat', '326.dat', '327.dat', '328.dat', '329.dat', '330.dat', '331.dat', '332.dat', '333.dat', '334.dat', '335.dat', '336.dat', '337.dat', '338.dat', '339.dat', '340.dat', '341.dat', '342.dat', '343.dat', '344.dat', '345.dat', '346.dat', '347.dat', '348.dat', '349.dat', '350.dat', '351.dat', '352.dat', '353.dat', '354.dat', '355.dat', '356.dat', '357.dat', '358.dat', '359.dat', '360.dat', '361.dat', '362.dat', '363.dat', '364.dat', '365.dat', '366.dat', '367.dat', '368.dat', '369.dat', '370.dat', '371.dat', '372.dat', '373.dat', '374.dat', '375.dat', '376.dat', '377.dat', '378.dat', '379.dat', '380.dat', '381.dat', '382.dat', '383.dat', '384.dat', '385.dat', '386.dat', '387.dat', '388.dat', '389.dat', '390.dat', '391.dat', '392.dat', '393.dat', '394.dat', '395.dat', '396.dat', '397.dat', '398.dat', '399.dat', '400.dat', '401.dat', '402.dat', '403.dat', '404.dat', '405.dat', '406.dat', '407.dat', '408.dat', '409.dat', '410.dat', '411.dat', '412.dat', '413.dat', '414.dat', '415.dat', '416.dat', '417.dat', '418.dat', '419.dat', '420.dat', '421.dat', '422.dat', '423.dat', '424.dat', '425.dat', '426.dat', '427.dat', '428.dat', '429.dat', '430.dat', '431.dat', '432.dat', '433.dat', '434.dat', '435.dat', '436.dat', '437.dat', '438.dat', '439.dat', '440.dat', '441.dat', '442.dat', '443.dat', '444.dat', '445.dat', '446.dat', '447.dat', '448.dat', '449.dat', '450.dat', '451.dat', '452.dat', '453.dat', '454.dat', '455.dat', '456.dat', '457.dat', '458.dat', '459.dat', '460.dat', '461.dat', '462.dat', '463.dat', '464.dat', '465.dat', '466.dat', '467.dat', '468.dat', '469.dat', '470.dat', '471.dat', '472.dat', '473.dat', '474.dat', '475.dat', '476.dat', '477.dat', '478.dat', '479.dat', '480.dat', '481.dat', '482.dat', '483.dat', '484.dat', '485.dat', '486.dat', '487.dat', '488.dat', '489.dat', '490.dat', '491.dat', '492.dat', '493.dat', '494.dat', '495.dat', '496.dat', '497.dat', '498.dat', '499.dat', '500.dat', '501.dat', '502.dat', '503.dat', '504.dat', '505.dat', '506.dat', '507.dat', '508.dat', '509.dat', '510.dat', '511.dat', '512.dat', '513.dat', '514.dat', '515.dat', '516.dat', '517.dat', '518.dat', '519.dat', '520.dat', '521.dat', '522.dat', '523.dat', '524.dat', '525.dat', '526.dat', '527.dat', '528.dat', '529.dat', '530.dat', '531.dat', '532.dat', '533.dat', '534.dat', '535.dat', '536.dat', '537.dat', '538.dat', '539.dat', '540.dat', '541.dat', '542.dat', '543.dat', '544.dat', '545.dat', '546.dat', '547.dat', '548.dat', '549.dat', '550.dat', '551.dat', '552.dat', '553.dat', '554.dat', '555.dat', '556.dat', '557.dat', '558.dat', '559.dat', '560.dat', '561.dat', '562.dat', '563.dat', '564.dat', '565.dat', '566.dat', '567.dat', '568.dat', '569.dat', '570.dat', '571.dat', '572.dat', '573.dat', '574.dat', '575.dat', '576.dat', '577.dat', '578.dat', '579.dat', '580.dat', '581.dat', '582.dat', '583.dat', '584.dat', '585.dat', '586.dat', '587.dat', '588.dat', '589.dat', '590.dat', '591.dat', '592.dat', '593.dat', '594.dat', '595.dat', '596.dat', '597.dat', '598.dat', '599.dat', '600.dat', '601.dat', '602.dat', '603.dat', '604.dat', '605.dat', '606.dat', '607.dat', '608.dat', '609.dat', '610.dat', '611.dat', '612.dat', '613.dat', '614.dat', '615.dat', '616.dat', '617.dat', '618.dat', '619.dat', '620.dat', '621.dat', '622.dat', '623.dat', '624.dat', '625.dat', '626.dat', '627.dat', '628.dat', '629.dat', '630.dat', '631.dat', '632.dat', '633.dat', '634.dat', '635.dat', '636.dat', '637.dat', '638.dat', '639.dat', '640.dat', '641.dat', '642.dat', '643.dat', '644.dat', '645.dat', '646.dat', '647.dat', '648.dat', '649.dat', '650.dat', '651.dat', '652.dat', '653.dat', '654.dat', '655.dat', '656.dat', '657.dat', '658.dat', '659.dat', '660.dat', '661.dat', '662.dat', '663.dat', '664.dat', '665.dat', '666.dat', '667.dat', '668.dat', '669.dat', '670.dat', '671.dat', '672.dat', '673.dat', '674.dat', '675.dat']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dat_folder = \"sample_tile_results3\"\n",
    "\n",
    "# List all .dat files not starting with 'file'\n",
    "dat_files = [\n",
    "    f for f in os.listdir(dat_folder)\n",
    "    if f.endswith(\".dat\") and not f.startswith(\"file\")\n",
    "]\n",
    "\n",
    "print(f\"Found {len(dat_files)} .dat files (excluding those starting with 'file'):\")\n",
    "print(dat_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eab71454-c3db-4e80-809a-58965f07aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             nuc_id  type type_name        bounding_box  \\\n",
      "0  5772d1d5819e4685bd306c1b7cb12c67     1     Tumor  [134, 39, 151, 56]   \n",
      "1  56a9bb390ca74e2ca64eb9dfb4dbcec0     1     Tumor    [36, 42, 66, 62]   \n",
      "2  48f6b51d11894dfe806da774b31ff851     1     Tumor   [99, 55, 127, 71]   \n",
      "3  6c3f6c9722ca41f5860f8c123f50da2e     1     Tumor  [135, 62, 150, 80]   \n",
      "4  7abb3f9c76814a079cbe7328fb474d4e     1     Tumor  [154, 64, 174, 82]   \n",
      "\n",
      "                                  centroid      prob  \n",
      "0  [142.0990990990991, 47.387387387387385]  0.972973  \n",
      "1   [49.3448275862069, 50.716180371352785]  0.870027  \n",
      "2  [112.22402597402598, 62.40909090909091]  0.957792  \n",
      "3  [142.08415841584159, 70.03960396039604]  0.910891  \n",
      "4    [164.0326530612245, 72.2204081632653]  0.979592  \n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "# Mapping of type labels\n",
    "type_mapping = {\n",
    "    0: \"Background\",\n",
    "    1: \"Tumor\",\n",
    "    2: \"Immune\",\n",
    "    3: \"Inflammatory\",\n",
    "    4: \"Connective\",\n",
    "    5: \"Dead\"\n",
    "}\n",
    "Master_nuclei_data = []\n",
    "\n",
    "for data in dat_files:\n",
    "    file_number = os.path.splitext(data)[0]  # e.g., '555' from '555.dat'\n",
    "    try:\n",
    "        tile_preds = joblib.load(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {data}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Load a single .dat file\n",
    "    tile_dat_file = \"sample_tile_results3/555.dat\"\n",
    "    tile_preds = joblib.load(tile_dat_file)\n",
    "\n",
    "    Master_nuclei_data = []\n",
    "\n",
    "    \n",
    "    nuc_id_list = list(tile_preds.keys())\n",
    "    \n",
    "    for i in range(len(nuc_id_list)):\n",
    "        selected_nuc_id = nuc_id_list[i]\n",
    "        sample_nuc = tile_preds[selected_nuc_id]\n",
    "    \n",
    "        nuc_id = selected_nuc_id\n",
    "        cell_type_id = sample_nuc[\"type\"]\n",
    "        cell_type_name = type_mapping.get(cell_type_id, \"Unknown\")\n",
    "    \n",
    "        bounding_box = [\n",
    "            sample_nuc[\"box\"][0],\n",
    "            sample_nuc[\"box\"][1],\n",
    "            sample_nuc[\"box\"][2],\n",
    "            sample_nuc[\"box\"][3]\n",
    "        ]\n",
    "    \n",
    "        centroid = [\n",
    "            sample_nuc[\"centroid\"][0],\n",
    "            sample_nuc[\"centroid\"][1]\n",
    "        ]\n",
    "    \n",
    "        probability = sample_nuc[\"prob\"]\n",
    "    \n",
    "        nuclei_data.append({\n",
    "            \"nuc_id\": nuc_id,\n",
    "            \"type\": cell_type_id,\n",
    "            \"type_name\": cell_type_name,\n",
    "            \"bounding_box\": bounding_box,\n",
    "            \"centroid\": centroid,\n",
    "            \"prob\": probability\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(nuclei_data)\n",
    "\n",
    "# Optional: save to CSV\n",
    "# df.to_csv(\"tile_555_nuclei_data.csv\", index=False)\n",
    "\n",
    "# Show preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "278d0623-5b6f-4ffc-9873-8d96bc89493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nuclei processed: 27743\n",
      "  file_number                            nuc_id  type     type_name  \\\n",
      "0           0  ebe6bc1ea6b544aca2786a5e7c73bb4e     3  Inflammatory   \n",
      "1           0  9fbd4c54b7d74e98b633cec629eb8d4a     3  Inflammatory   \n",
      "2           1  9c4cfd1a3b7642c9a0288de6382ae789     3  Inflammatory   \n",
      "3           1  6f90c615363c4632a3d851f3cbe83ee4     3  Inflammatory   \n",
      "4           1  13ff7f24d6a7417aaf3f7bfbd39ca551     3  Inflammatory   \n",
      "\n",
      "           bounding_box                                  centroid      prob  \n",
      "0    [117, 39, 126, 56]   [121.00869565217391, 47.19130434782609]  1.000000  \n",
      "1    [174, 62, 184, 76]   [178.43119266055047, 68.98165137614679]  0.917431  \n",
      "2      [17, 19, 28, 52]   [21.835748792270532, 35.56038647342995]  0.990338  \n",
      "3    [133, 37, 144, 58]    [137.63522012578616, 46.9496855345912]  0.987421  \n",
      "4  [178, 149, 189, 171]  [182.94160583941607, 158.43065693430657]  1.000000  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Folder path\n",
    "dat_folder = \"sample_tile_results3\"\n",
    "\n",
    "# Get all .dat files excluding ones starting with \"file\"\n",
    "dat_files = [\n",
    "    os.path.join(dat_folder, f)\n",
    "    for f in os.listdir(dat_folder)\n",
    "    if f.endswith(\".dat\") and not f.startswith(\"file\")\n",
    "]\n",
    "\n",
    "# Mapping of type labels\n",
    "type_mapping = {\n",
    "    0: \"Background\",\n",
    "    1: \"Tumor\",\n",
    "    2: \"Immune\",\n",
    "    3: \"Inflammatory\",\n",
    "    4: \"Connective\",\n",
    "    5: \"Dead\"\n",
    "}\n",
    "\n",
    "# Master list to collect all nuclei data\n",
    "Master_nuclei_data = []\n",
    "\n",
    "# Loop over each valid .dat file\n",
    "for data in dat_files:\n",
    "    file_number = os.path.splitext(os.path.basename(data))[0]  # e.g., '555'\n",
    "\n",
    "    try:\n",
    "        tile_preds = joblib.load(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {data}: {e}\")\n",
    "        continue\n",
    "\n",
    "    nuc_id_list = list(tile_preds.keys())\n",
    "\n",
    "    for nuc_id in nuc_id_list:\n",
    "        sample_nuc = tile_preds[nuc_id]\n",
    "\n",
    "        cell_type_id = sample_nuc.get(\"type\")\n",
    "        cell_type_name = type_mapping.get(cell_type_id, \"Unknown\")\n",
    "\n",
    "        bounding_box = sample_nuc.get(\"box\", [None, None, None, None])\n",
    "        centroid = sample_nuc.get(\"centroid\", [None, None])\n",
    "        sample_nuc[\"contour\"]\n",
    "        probability = sample_nuc.get(\"prob\")\n",
    "\n",
    "        Master_nuclei_data.append({\n",
    "            \"file_number\": file_number,\n",
    "            \"nuc_id\": nuc_id,\n",
    "            \"type\": cell_type_id,\n",
    "            \"type_name\": cell_type_name,\n",
    "            \"bounding_box\": bounding_box,\n",
    "            \"centroid\": centroid,\n",
    "            \n",
    "            \"prob\": probability\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(Master_nuclei_data)\n",
    "\n",
    "# Optional: save\n",
    "df.to_csv(\"all_nuclei_data.csv\", index=False)\n",
    "\n",
    "print(f\"Total nuclei processed: {len(df)}\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47137242-b055-4a20-9584-a8932bf0f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nuclei processed: 27743\n",
      "  file_number                            nuc_id  type     type_name  \\\n",
      "0           0  ebe6bc1ea6b544aca2786a5e7c73bb4e     3  Inflammatory   \n",
      "1           0  9fbd4c54b7d74e98b633cec629eb8d4a     3  Inflammatory   \n",
      "2           1  9c4cfd1a3b7642c9a0288de6382ae789     3  Inflammatory   \n",
      "3           1  6f90c615363c4632a3d851f3cbe83ee4     3  Inflammatory   \n",
      "4           1  13ff7f24d6a7417aaf3f7bfbd39ca551     3  Inflammatory   \n",
      "\n",
      "           bounding_box                                  centroid      prob  \\\n",
      "0    [117, 39, 126, 56]   [121.00869565217391, 47.19130434782609]  1.000000   \n",
      "1    [174, 62, 184, 76]   [178.43119266055047, 68.98165137614679]  0.917431   \n",
      "2      [17, 19, 28, 52]   [21.835748792270532, 35.56038647342995]  0.990338   \n",
      "3    [133, 37, 144, 58]    [137.63522012578616, 46.9496855345912]  0.987421   \n",
      "4  [178, 149, 189, 171]  [182.94160583941607, 158.43065693430657]  1.000000   \n",
      "\n",
      "                                             contour  \n",
      "0  [[119, 39], [117, 41], [117, 48], [118, 49], [...  \n",
      "1  [[178, 62], [177, 63], [176, 63], [176, 64], [...  \n",
      "2  [[25, 19], [23, 21], [23, 22], [22, 23], [22, ...  \n",
      "3  [[139, 37], [138, 38], [137, 38], [135, 40], [...  \n",
      "4  [[184, 149], [181, 152], [181, 154], [180, 155...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Folder path\n",
    "dat_folder = \"sample_tile_results3\"\n",
    "\n",
    "# Get all .dat files excluding those starting with \"file\"\n",
    "dat_files = [\n",
    "    os.path.join(dat_folder, f)\n",
    "    for f in os.listdir(dat_folder)\n",
    "    if f.endswith(\".dat\") and not f.startswith(\"file\")\n",
    "]\n",
    "\n",
    "# Mapping of type labels\n",
    "type_mapping = {\n",
    "    0: \"Background\",\n",
    "    1: \"Tumor\",\n",
    "    2: \"Immune\",\n",
    "    3: \"Inflammatory\",\n",
    "    4: \"Connective\",\n",
    "    5: \"Dead\"\n",
    "}\n",
    "\n",
    "# Master list to collect all nuclei data\n",
    "Master_nuclei_data = []\n",
    "\n",
    "# Loop over each valid .dat file\n",
    "for data in dat_files:\n",
    "    file_number = os.path.splitext(os.path.basename(data))[0]  # e.g., '555'\n",
    "\n",
    "    try:\n",
    "        tile_preds = joblib.load(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {data}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for nuc_id, sample_nuc in tile_preds.items():\n",
    "        cell_type_id = sample_nuc.get(\"type\")\n",
    "        cell_type_name = type_mapping.get(cell_type_id, \"Unknown\")\n",
    "\n",
    "        bounding_box = sample_nuc.get(\"box\", [None, None, None, None])\n",
    "        centroid = sample_nuc.get(\"centroid\", [None, None])\n",
    "        probability = sample_nuc.get(\"prob\")\n",
    "\n",
    "        # Convert contour to plain list of [x, y]\n",
    "        raw_contour = sample_nuc.get(\"contour\", [])\n",
    "        contour = [list(point) for point in raw_contour]  # Ensure it's JSON/CSV serializable\n",
    "\n",
    "        Master_nuclei_data.append({\n",
    "            \"file_number\": file_number,\n",
    "            \"nuc_id\": nuc_id,\n",
    "            \"type\": cell_type_id,\n",
    "            \"type_name\": cell_type_name,\n",
    "            \"bounding_box\": bounding_box,\n",
    "            \"centroid\": centroid,\n",
    "            \"prob\": probability,\n",
    "            \"contour\": contour\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(Master_nuclei_data)\n",
    "\n",
    "# Optional: Save to CSV (convert contour to string format for CSV)\n",
    "df.to_csv(\"all_nuclei_data_with_contour.csv\", index=False)\n",
    "\n",
    "# Display result\n",
    "print(f\"Total nuclei processed: {len(df)}\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "589c0727-9bfd-4300-b75c-00b78d47b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Generating tumor overlays from matched files...\n",
      "✅ Tumor overlays saved.\n",
      "\n",
      "🧩 Stitching tiles into full ROI...\n",
      "✅ Full ROI with tumor contours saved to:\n",
      "➡️ /lab/deasylab3/Himangi/tnbc_data/stitched_roi_with_tumor_contours.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "dat_folder = \"sample_tile_results3\"       # Folder with .dat files\n",
    "tile_image_folder = \"tiles\"         # Folder with original tile images (224x224)\n",
    "overlay_folder = \"/lab/deasylab3/Himangi/tnbc_data/tile_overlays\"          # Output folder for contour overlays\n",
    "output_path = \"/lab/deasylab3/Himangi/tnbc_data/stitched_roi_with_tumor_contours.png\"  # Final stitched ROI\n",
    "TILE_SIZE = 224\n",
    "TUMOR_TYPE_ID = 1\n",
    "\n",
    "os.makedirs(overlay_folder, exist_ok=True)\n",
    "\n",
    "# --- STEP 1: Get matching .png and .dat files sorted ---\n",
    "dat_files = sorted(\n",
    "    [f for f in os.listdir(dat_folder) if f.endswith(\".dat\") and not f.startswith(\"file\")],\n",
    "    key=lambda x: int(os.path.splitext(x)[0])\n",
    ")\n",
    "\n",
    "png_files = sorted(\n",
    "    [f for f in os.listdir(tile_image_folder) if f.endswith(\".png\") and \"x\" in f and \"y\" in f],\n",
    "    key=lambda x: int(re.search(r\"patch_(\\d+)\", x).group(1))\n",
    ")\n",
    "\n",
    "if len(dat_files) != len(png_files):\n",
    "    raise ValueError(f\"Mismatch: {len(dat_files)} .dat files vs {len(png_files)} .png tiles\")\n",
    "\n",
    "tile_positions = {}\n",
    "max_x = max_y = 0\n",
    "\n",
    "# --- STEP 2: Process each tile and overlay contours ---\n",
    "print(\"🔄 Generating tumor overlays from matched files...\")\n",
    "\n",
    "for idx, (dat_file, png_file) in enumerate(zip(dat_files, png_files)):\n",
    "    dat_path = os.path.join(dat_folder, dat_file)\n",
    "    png_path = os.path.join(tile_image_folder, png_file)\n",
    "    tile_id = os.path.splitext(png_file)[0]\n",
    "    out_path = os.path.join(overlay_folder, f\"{tile_id}.png\")\n",
    "\n",
    "    # Extract x/y from PNG filename\n",
    "    match = re.search(r\"x(\\d+)_y(\\d+)\", png_file)\n",
    "    if not match:\n",
    "        print(f\"⚠️ Skipping malformed filename: {png_file}\")\n",
    "        continue\n",
    "\n",
    "    x_offset = int(match.group(1))\n",
    "    y_offset = int(match.group(2))\n",
    "\n",
    "    tile_positions[tile_id] = (x_offset, y_offset)\n",
    "    max_x = max(max_x, x_offset)\n",
    "    max_y = max(max_y, y_offset)\n",
    "\n",
    "    try:\n",
    "        preds = joblib.load(dat_path)\n",
    "        img = cv2.imread(png_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {dat_file} or {png_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    overlay = img.copy()\n",
    "\n",
    "    for nuc_id, nuc in preds.items():\n",
    "        if not isinstance(nuc, dict) or nuc.get(\"type\") != TUMOR_TYPE_ID:\n",
    "            continue\n",
    "\n",
    "        contour = np.array(nuc.get(\"contour\", []), dtype=np.int32)\n",
    "        if contour.size == 0:\n",
    "            continue\n",
    "\n",
    "        contour = contour.reshape((-1, 1, 2))\n",
    "        cv2.drawContours(overlay, [contour], -1, (255, 0, 0), 1)\n",
    "\n",
    "    cv2.imwrite(out_path, overlay)\n",
    "\n",
    "print(\"✅ Tumor overlays saved.\\n\")\n",
    "\n",
    "# --- STEP 3: Stitch overlays using (x_offset, y_offset) ---\n",
    "stitched_width = max_x + TILE_SIZE\n",
    "stitched_height = max_y + TILE_SIZE\n",
    "stitched = np.zeros((stitched_height, stitched_width, 3), dtype=np.uint8)\n",
    "\n",
    "print(\"🧩 Stitching tiles into full ROI...\")\n",
    "\n",
    "for tile_id, (x_offset, y_offset) in tile_positions.items():\n",
    "    tile_path = os.path.join(overlay_folder, f\"{tile_id}.png\")\n",
    "    if not os.path.exists(tile_path):\n",
    "        continue\n",
    "\n",
    "    tile_img = cv2.imread(tile_path)\n",
    "    stitched[y_offset:y_offset+TILE_SIZE, x_offset:x_offset+TILE_SIZE] = tile_img\n",
    "\n",
    "cv2.imwrite(output_path, stitched)\n",
    "print(f\"✅ Full ROI with tumor contours saved to:\\n➡️ {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a5e05a5-50bb-4474-8595-75d165d1736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_dat_file = \"sample_tile_results3/555.dat\"\n",
    "tile_preds = joblib.load(tile_dat_file)\n",
    "nuc_id_list = list(tile_preds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "905cfc24-6926-41e0-9c92-528474e39273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Number of detected nuclei: 32\n",
      "INFO:root:Keys in the output dictionary: [box, centroid, contour, prob, type]\n",
      "INFO:root:Bounding box: (70, 95, 87, 112)\n",
      "INFO:root:Centroid: (77, 102)\n",
      "INFO:root:Type label: 1\n",
      "INFO:root:Type contour: [[ 78  95]\n",
      " [ 77  96]\n",
      " [ 76  96]\n",
      " [ 75  97]\n",
      " [ 74  97]\n",
      " [ 73  98]\n",
      " [ 73  99]\n",
      " [ 70 102]\n",
      " [ 70 108]\n",
      " [ 73 111]\n",
      " [ 74 111]\n",
      " [ 75 110]\n",
      " [ 78 110]\n",
      " [ 79 109]\n",
      " [ 80 109]\n",
      " [ 83 106]\n",
      " [ 83 105]\n",
      " [ 84 104]\n",
      " [ 84 103]\n",
      " [ 85 102]\n",
      " [ 85 101]\n",
      " [ 86 100]\n",
      " [ 86  97]\n",
      " [ 85  97]\n",
      " [ 83  95]]\n",
      "INFO:root:prob: 0.9607843090157632\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Load .dat file from HoverNet output\n",
    "tile_dat_file = \"sample_tile_results3/555.dat\"\n",
    "tile_preds = joblib.load(tile_dat_file)\n",
    "\n",
    "logger.info(f\"Number of detected nuclei: {len(tile_preds)}\")\n",
    "\n",
    "# Pick the first nucleus\n",
    "nuc_id_list = list(tile_preds.keys())\n",
    "selected_nuc_id = nuc_id_list[12]\n",
    "\n",
    "sample_nuc = tile_preds[selected_nuc_id]\n",
    "sample_nuc_keys = list(sample_nuc.keys())\n",
    "\n",
    "logger.info(\n",
    "    \"Keys in the output dictionary: [%s]\",\n",
    "    \", \".join(sample_nuc_keys)\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"Bounding box: (%d, %d, %d, %d)\",\n",
    "    sample_nuc[\"box\"][0],\n",
    "    sample_nuc[\"box\"][1],\n",
    "    sample_nuc[\"box\"][2],\n",
    "    sample_nuc[\"box\"][3],\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"Centroid: (%d, %d)\",\n",
    "    sample_nuc[\"centroid\"][0],\n",
    "    sample_nuc[\"centroid\"][1],\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"Type label: %s\",\n",
    "    sample_nuc[\"type\"]\n",
    ")\n",
    "\n",
    "\n",
    "logger.info(\n",
    "    \"Type contour: %s\",\n",
    "    sample_nuc[\"contour\"]\n",
    ")\n",
    "\n",
    "\n",
    "logger.info(\n",
    "    \"prob: %s\",\n",
    "    sample_nuc[\"prob\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e5432f04-2415-4164-b080-160188242644",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nuc_id, nuc_info \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtile_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m     24\u001b[0m     row \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: tile_id,\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: nuc_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: nuc_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     }\n\u001b[1;32m     32\u001b[0m     nuclei_data\u001b[38;5;241m.\u001b[39mappend(row)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing .dat files\n",
    "dat_folder = \"sample_tile_results3\"\n",
    "\n",
    "# Initialize list to hold all rows\n",
    "nuclei_data = []\n",
    "\n",
    "# Iterate through all .dat files\n",
    "for filename in os.listdir(dat_folder):\n",
    "    if filename.endswith(\".dat\"):\n",
    "        filepath = os.path.join(dat_folder, filename)\n",
    "        tile_id = os.path.splitext(filename)[0]  # e.g., '555' from '555.dat'\n",
    "\n",
    "        try:\n",
    "            tile_preds = joblib.load(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for nuc_id, nuc_info in tile_preds.items():\n",
    "            row = {\n",
    "                \"file_number\": tile_id,\n",
    "                \"nuc_id\": nuc_id,\n",
    "                \"type\": nuc_info.get(\"type\"),\n",
    "                \"bounding_box\": tuple(nuc_info.get(\"box\", (None, None, None, None))),\n",
    "                \"contour\": nuc_info.get(\"contour\"),\n",
    "                \"prob\": nuc_info.get(\"prob\")\n",
    "            }\n",
    "            nuclei_data.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(nuclei_data)\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# df.to_csv(\"nuclei_summary.csv\", index=False)\n",
    "\n",
    "print(f\"Loaded data for {len(df)} nuclei across {df['file_number'].nunique()} tiles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5f8ef0d-a6e0-4c7d-986f-eb5570b9f598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tumor nuclei count: 26\n"
     ]
    }
   ],
   "source": [
    "tumor_class = 1  # tumor in PanNuke\n",
    "\n",
    "tumor_nuclei = {\n",
    "    nid: info\n",
    "    for nid, info in tile_preds.items()\n",
    "    if info[\"type\"] == tumor_class\n",
    "}\n",
    "\n",
    "logger.info(f\"Tumor nuclei count: {len(tumor_nuclei)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12bc4ac3-e32e-4df0-80d2-98df6caa899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_patches = []\n",
    "for nid, info in tumor_nuclei.items():\n",
    "    x, y = info[\"centroid\"]\n",
    "    x1 = max(0, x - 224 // 2)\n",
    "    y1 = max(0, y - 224 // 2)\n",
    "    x2 = x1 + 224\n",
    "    y2 = y1 + 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6aec8b1-9fc8-4478-997e-96505353b096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.09909909999999"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "142.0990991-224 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dfb990a1-c533-4844-8851-d12cee1eb7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.099099099099107 0\n",
      "0 0\n",
      "0.22402597402597735 0\n",
      "30.084158415841586 0\n",
      "52.032653061224494 0\n",
      "72.15517241379311 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "13.087855297157617 0\n",
      "52.74038461538461 0\n",
      "0 0\n",
      "9.64577656675749 8.460490463215251\n",
      "0 3.138686131386862\n",
      "35.58019801980197 14.986138613861385\n",
      "0 19.73975903614459\n",
      "0 21.68965517241378\n",
      "58.542124542124554 24.849816849816847\n",
      "0 39.206303724928375\n",
      "0 55.926315789473676\n",
      "19.30857142857144 60.54857142857142\n",
      "0 64.0811965811966\n",
      "10.302238805970148 76.04850746268656\n",
      "0 76.74799999999999\n",
      "0 93.7439024390244\n",
      "0 98.0084745762712\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for nid, info in tumor_nuclei.items():\n",
    "    x, y = info[\"centroid\"]\n",
    "    x1 = max(0, x - 224 // 2)\n",
    "    y1 = max(0, y - 224 // 2)\n",
    "    print(x1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66118414-fdf3-4d00-a13e-f8ac9f190ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
