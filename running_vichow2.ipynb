{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ce9049-708b-425d-8e0a-8a776c1e48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.layers import SwiGLUPacked\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4507d9-3182-44e4-b60e-e94e23c96822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4156c4b2ad4c9cba84aca42f6d13db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d0db3d-4bb2-415d-9da3-b31edb68094b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import glob\n",
    "png_paths = sorted(glob.glob(os.path.join(\"clean_tiles\", \"*.png\")))\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68576c93-6a81-4f17-a431-a17579c56b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"hf-hub:paige-ai/Virchow2\", pretrained=True, mlp_layer=SwiGLUPacked, act_layer=torch.nn.SiLU)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d0b57c-1ee1-4c0c-acfd-2a03fa81b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce007740-dc88-4c6d-83ee-eea1db4c776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "png_paths = sorted(glob.glob(os.path.join(\"clean_tiles\", \"*.png\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878df987-ed6b-4b78-8396-88ea255efff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.cluster import DBSCAN\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.layers import SwiGLUPacked\n",
    "import pandas as pd\n",
    "import ace_tools as tools\n",
    "\n",
    "# Load Virchow2 model\n",
    "model = timm.create_model(\"hf-hub:paige-ai/Virchow2\", pretrained=True,\n",
    "                          mlp_layer=SwiGLUPacked, act_layer=torch.nn.SiLU)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Get transform\n",
    "transforms = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\n",
    "\n",
    "# Get tile paths\n",
    "tile_paths = sorted(glob.glob(\"clean_tiles/*.png\"))\n",
    "\n",
    "# Extract per-tile embeddings\n",
    "tile_embeddings = []\n",
    "tile_coords = []\n",
    "slide_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path in tqdm(tile_paths, desc=\"Extracting embeddings\"):\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        tensor = transforms(image).unsqueeze(0).to(device)\n",
    "        output = model(tensor)\n",
    "\n",
    "        class_token = output[:, 0]\n",
    "        patch_tokens = output[:, 5:]\n",
    "        embedding = torch.cat([class_token, patch_tokens.mean(1)], dim=-1)\n",
    "\n",
    "        tile_embeddings.append(embedding.squeeze(0).cpu())\n",
    "\n",
    "        # Extract slide ID and coordinates\n",
    "        filename = os.path.basename(path)\n",
    "        parts = filename.replace(\".png\", \"\").split(\"_\")[-2:]\n",
    "        #slide_id = parts[0]\n",
    "        x = int(parts[0])  # e.g., x0 -> 0\n",
    "        y = int(parts[1])  # e.g., y0 -> 0\n",
    "\n",
    "        #slide_ids.append(slide_id)\n",
    "        tile_coords.append((x, y))\n",
    "\n",
    "# Convert to tensor\n",
    "X = torch.stack(tile_embeddings)\n",
    "X_np = X.numpy()\n",
    "\n",
    "# UMAP reduction\n",
    "umap_2d = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine', random_state=42)\n",
    "X_umap = umap_2d.fit_transform(X_np)\n",
    "\n",
    "# DBSCAN clustering\n",
    "clustering = DBSCAN(eps=0.3, min_samples=5).fit(X_umap)\n",
    "cluster_labels = clustering.labels_\n",
    "\n",
    "# Prepare per-slide pooled embeddings\n",
    "slide_to_tiles = {}\n",
    "for emb, sid in zip(X, slide_ids):\n",
    "    slide_to_tiles.setdefault(sid, []).append(emb)\n",
    "\n",
    "slide_pooled = {sid: torch.stack(tiles).mean(0) for sid, tiles in slide_to_tiles.items()}\n",
    "\n",
    "# Visualization heatmaps for each slide\n",
    "heatmap_data = []\n",
    "for sid in set(slide_ids):\n",
    "    coords = [(x, y, label) for (s, (x, y), label) in zip(slide_ids, tile_coords, cluster_labels) if s == sid]\n",
    "    if not coords:\n",
    "        continue\n",
    "    xs, ys, lbls = zip(*coords)\n",
    "    max_x, max_y = max(xs) + 1, max(ys) + 1\n",
    "    grid = np.full((max_y, max_x), -1)\n",
    "    for x, y, lbl in coords:\n",
    "        grid[y, x] = lbl\n",
    "\n",
    "    # Save heatmap image to disk (in-memory visualization)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(grid, cmap=\"tab10\", interpolation=\"nearest\")\n",
    "    plt.title(f\"Cluster heatmap: {sid}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{sid}_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "    heatmap_data.append({\n",
    "        \"slide_id\": sid,\n",
    "        \"heatmap_path\": f\"{sid}_heatmap.png\"\n",
    "    })\n",
    "\n",
    "# Output table with slide IDs and heatmap paths\n",
    "df = pd.DataFrame(heatmap_data)\n",
    "tools.display_dataframe_to_user(name=\"Slide Heatmaps\", dataframe=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728e305b-850f-4ff3-8ecc-3f719c49e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "device =\"cpu\"\n",
    "def extract_deep_features_as_npy(tile_paths, output_dir=\"tile_npy_embeddings\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    tile_coords = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for path in tqdm(tile_paths, desc=\"Extracting & saving tile embeddings\"):\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            tensor = transforms(image).unsqueeze(0).to(device)\n",
    "            output = model(tensor)\n",
    "\n",
    "            class_token = output[:, 0]\n",
    "            patch_tokens = output[:, 5:]\n",
    "            embedding = torch.cat([class_token, patch_tokens.mean(1)], dim=-1)\n",
    "\n",
    "            # Extract filename and coordinates\n",
    "            filename = os.path.basename(path).replace(\".png\", \"\")\n",
    "            parts = filename.split(\"_\")[-2:]\n",
    "            x = int(parts[0])\n",
    "            y = int(parts[1])\n",
    "            tile_coords.append((x, y))\n",
    "\n",
    "            # Save as individual .npy file\n",
    "            save_path = os.path.join(output_dir, f\"{filename}.npy\")\n",
    "            np.save(save_path, embedding.squeeze(0).cpu().numpy())\n",
    "\n",
    "    print(f\"Saved {len(tile_paths)} tile embeddings to '{output_dir}/'\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ddae5b-e545-442f-9f6d-32cea1c02bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting & saving tile embeddings: 100%|██████████████████████████████████████| 15297/15297 [3:26:47<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 15297 tile embeddings to 'deep_features/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_deep_features_as_npy(png_paths,\"deep_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609a32c1-b85a-4866-bbff-2e18d17b71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def compute_slide_embedding(tile_dir=\"tile_npy_embeddings\", pooling=\"mean\", output_path=\"slide_embedding.npy\"):\n",
    "    tile_embeddings = []\n",
    "\n",
    "    for fname in os.listdir(tile_dir):\n",
    "        if fname.endswith(\".npy\"):\n",
    "            emb = np.load(os.path.join(tile_dir, fname))\n",
    "            tile_embeddings.append(emb)\n",
    "\n",
    "    tile_embeddings = np.stack(tile_embeddings)\n",
    "\n",
    "    # Pool across all tiles\n",
    "    if pooling == \"mean\":\n",
    "        slide_embedding = tile_embeddings.mean(axis=0)\n",
    "    elif pooling == \"max\":\n",
    "        slide_embedding = tile_embeddings.max(axis=0)\n",
    "    elif pooling == \"median\":\n",
    "        slide_embedding = np.median(tile_embeddings, axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pooling method: {pooling}\")\n",
    "\n",
    "    # Save the result\n",
    "    np.save(output_path, slide_embedding)\n",
    "    print(f\"Saved whole-slide embedding to: {output_path}\")\n",
    "\n",
    "    return slide_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af67800-d7e7-4bad-ac68-bb5f2b1ca6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from PIL import Image\n",
    "\n",
    "def umap_reducer(x: np.ndarray, dims: int = 3, nns: int = 10) -> np.ndarray:\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=nns,\n",
    "        n_components=dims,\n",
    "        metric=\"manhattan\",\n",
    "        spread=0.5,\n",
    "        random_state=2,\n",
    "    )\n",
    "    reduced = reducer.fit_transform(x)\n",
    "    reduced -= reduced.min(axis=0)\n",
    "    reduced /= reduced.max(axis=0)\n",
    "    return reduced\n",
    "\n",
    "def scatter_tile_umap(tile_dir, output_path, thumbnail_path=None):\n",
    "    tile_paths = sorted(glob.glob(os.path.join(tile_dir, \"tile_*.npy\")))\n",
    "    embeddings, coords = [], []\n",
    "\n",
    "    for path in tile_paths:\n",
    "        base = os.path.basename(path).replace(\".npy\", \"\")\n",
    "        _, _, x, y = base.split(\"_\")\n",
    "        x, y = int(x), int(y)\n",
    "        coords.append((x, y))\n",
    "        embeddings.append(np.load(path))\n",
    "\n",
    "    coords = np.array(coords)\n",
    "    embeddings = np.stack(embeddings)\n",
    "    reduced_colors = umap_reducer(embeddings)  # shape: (N, 3)\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    if thumbnail_path:\n",
    "        thumbnail = Image.open(thumbnail_path)\n",
    "        plt.imshow(thumbnail)\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], c=reduced_colors, s=5, alpha=0.6)\n",
    "    plt.gca().invert_yaxis()  # optional: match image convention\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"UMAP Feature Embedding (Scatter View)\")\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[✓] Saved scatter UMAP view to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "004a901c-f000-4cb5-83f1-6bce83376df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/srivash/miniforge3/envs/tiatoolbox/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Saved scatter UMAP view to: tile_umap_scatter.png\n"
     ]
    }
   ],
   "source": [
    "scatter_tile_umap(\n",
    "    tile_dir=\"deep_features\",\n",
    "    output_path=\"tile_umap_scatter.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607f0ee-468f-4015-9d4e-c0acdc91f328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
